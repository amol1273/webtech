import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

x=np.array([[0,0],[0,1],[1,0],[1,1]])
y=np.array([0,1,1,0]) #XOR LOGIC

mlp = MLPClassifier(hidden_layer_sizes=(4, 4), max_iter=10000, activation='tanh', random_state=42)

mlp.fit(x,y)#train the model

y_pred = mlp.predict(x)
y_pred

accuracy = accuracy_score(y, y_pred)
print(f'Predictions: {y_pred}')
print(f'Accuracy: {accuracy * 100:.2f}%')

#Multi-layer Perceptron (MLP) is a type of artificial neural network that consists of multiple layers of neurons (or nodes), including:

#Input Layer: Receives the input features.
#Hidden Layers: One or more layers where computation occurs through weights, biases, and activation functions.
#Output Layer: Produces the final output (e.g., class probabilities for classification tasks).

#The activation parameter determines the activation function used in the neurons of the hidden layers.
#The activation function introduces non-linearity to the model, allowing it to learn complex patterns.

#relu (Rectified Linear Unit): Most commonly used. It outputs the input directly if positive; otherwise, it outputs zero.
â€‹#tanh (Hyperbolic Tangent): Outputs values between -1 and 1. It is zero-centered, helping with convergence.
#logistic (Sigmoid): Outputs values between 0 and 1. Used primarily in binary classification problems
